using Random: shuffle

export sgd!

function sgd!(network::Network, training_data, mini_batch_size, Î·, epochs=1)
    for _ in 1:epochs
        training_data = shuffle(training_data)
        mini_batches = Iterators.partition(training_data, mini_batch_size)
        for mini_batch in mini_batches
            update_mini_batch!(network, mini_batch, Î·)
        end
    end
    return network
end

function update_mini_batch!(network::Network, mini_batch, Î·)
    ğ¯w, ğ¯ğ—¯ = collect(zeros(size(weights)) for weights in network.weights),
    collect(zeros(size(biases)) for biases in network.biases)
    for (x, y) in mini_batch
        ğ¯wâ±, ğ¯ğ—¯â± = Backpropagator(sigmoid, sigmoidâ€²)(network, x, y)
        for j in eachindex(ğ¯w)
            ğ¯w[j][:, :] .+= ğ¯wâ±[j]
            ğ¯ğ—¯[j][:] .+= ğ¯ğ—¯â±[j]
        end
    end
    Î·â€² = Î· / length(mini_batch)
    # Update each layer's weights and biases
    for (wâ±¼â‚–, bâ±¼, âˆ‡wâ±¼â‚–, âˆ‡bâ±¼) in zip(network.weights, network.biases, ğ¯w, ğ¯ğ—¯)
        wâ±¼â‚–[:, :] .-= Î·â€² * âˆ‡wâ±¼â‚–
        bâ±¼[:] .-= Î·â€² * âˆ‡bâ±¼
    end
    return network
end
