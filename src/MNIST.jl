module MNIST

using ComputedFieldTypes: @computed
using Random: shuffle
using Statistics: mean

export Network, feedforward, computecost, sgd!

@computed struct Network{N}
    layers::NTuple{N,Int64}
    weights::NTuple{N - 1,Matrix{Float64}}
    biases::NTuple{N - 1,Vector{Float64}}
end
function Network(layers)
    weights = Tuple(
        Matrix{Float64}(undef, nj, nk) for
        (nj, nk) in zip(layers[2:end], layers[1:(end - 1)])
    )
    biases = Tuple(Vector{Float64}(undef, nj) for nj in layers[2:end])
    return Network{length(layers)}(layers, weights, biases)
end
Network(layers::Integer...) = Network(layers)

function feedforward(network::Network, ğš)
    for (w, ğ›) in (network.weights, network.biases)
        ğš = w * ğš .+ ğ›
    end
    return ğš
end

function computecost(network::Network, input, desired_output)
    output = feedforward(network, input)
    return sum(abs2, desired_output .- output)
end

function sgd!(network::Network, training_data, mini_batch_size, Î·, epochs=1)
    for _ in 1:epochs
        training_data = shuffle(training_data)
        mini_batches = Iterators.partition(training_data, mini_batch_size)
        for mini_batch in mini_batches
            update_mini_batch!(network, mini_batch, Î·)
        end
    end
    return network
end

function update_mini_batch!(network::Network, mini_batch, Î·)
    networks = map(mini_batch) do (x, y)
        backprop(network, x, y)  # For all layers
    end
    âˆ‡w, âˆ‡b = mean(network.weights for network in networks),
    mean(network.weights for network in networks)
    # Update each layer's weights and biases
    for (wâ±¼â‚–, bâ±¼, âˆ‡wâ±¼â‚–, âˆ‡bâ±¼) in zip(network.weights, network.biases, âˆ‡w, âˆ‡b)
        wâ±¼â‚–[:, :] .-= Î· * âˆ‡wâ±¼â‚–
        bâ±¼[:] .-= Î· * âˆ‡bâ±¼
    end
    return network
end

function backprop(network::Network, x, y) end

sigmoid(z) = 1 / (1 + exp(-z))

sigmoidâ€²(z) = sigmoid(z) * (1 - sigmoid(z))

end
